#!/usr/bin/env python
"""
Start an IPython.parallel cluster inside a PBS script.

Example PBS Script
------------------
#PBS -l nodes=2:ppn=16

pbsipcluster --daemonize   # start the cluster. 32 workers will boot

# Run your python scripts that connects to the workers and runs jobs
python -c 'import IPython.parallel; ...'

Notes
-----
If an existing cluster (ipcontroller) is running with the specified
cluster_id, this script will add new engines to that cluster, instead
of starting a new one.
"""
from __future__ import print_function, absolute_import, division
import os
import time
import socket
import argparse
import tempfile
import threading
import multiprocessing
from distutils.spawn import find_executable

import IPython.parallel
from IPython.utils.daemonize import daemonize


parser = argparse.ArgumentParser(description=__doc__,
                                 formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument('--unique',
                    help='Start only ipengine per node (use this if your '
                         'processes are\nindividually node-parallel).',
                    action='store_true', default=False)
parser.add_argument('-q', '--quiet', help='Quiet mode (default: False)',
                    action='store_true', default=False)
parser.add_argument('--daemonize', help='Daemonize (default: False)',
                    action='store_true', default=False)
parser.add_argument('-id', '--cluster-id',
                    help='Set the cluster ID to avoid collisions.')
parser.add_argument('-pre', '--pre-args',
                    help='Arguments executed on remote hosts before '
                         'ipengine start.\nOnly used with the ssh launcher.')
parser.add_argument('--launcher', choices=['ssh', 'mpi'], default='ssh',
                    help='launch the ipengines by ssh-ing to the nodes '
                    'or using the\nmpiexec launcher. Note that "ssh" '
                    'requires passwordless\nssh to be set up between the '
                    'nodes, while "mpi" requires a\nworking MPI installation.')
ARGS = parser.parse_args()


def log(msg):
    if not ARGS.quiet:
        print(msg)


def get_nodefile():
    own_nodefile = False

    if 'PBS_NODEFILE' in os.environ:
        if ARGS.unique:
            fid, nodefile = tempfile.mkstemp()
            own_nodefile = True
            os.close(fid)
            log('Getting unique entries in PBS_NODEFILE')
            with open(os.environ['PBS_NODEFILE']) as f:
                nodes = set(f.readlines())

            log('{:d} unique nodes')
            with open(nodefile, 'w') as f:
                for node in nodes:
                    f.write(node)
        else:
            nodefile = os.environ['PBS_NODEFILE']
    else:
        log('Not running under PBS')
        fid, nodefile = tempfile.mkstemp()
        own_nodefile = True
        os.close(fid)
        count = 1 if ARGS.unique else multiprocessing.cpu_count()
        with open(nodefile, 'w') as f:
            for _ in range(count):
                f.write('%s\n' % socket.gethostname())
    return nodefile, own_nodefile


def main():
    nodefile, own_nodefile = get_nodefile()

    q = '--quiet' if ARGS.quiet else ''
    if 'PBS_O_WORKDIR' in os.environ:
        work_dir = '--work-dir={}'.format(os.environ['PBS_O_WORKDIR'])
    else:
        work_dir = ''

    if ARGS.cluster_id:
        cluster_id = '--cluster-id={}'.format(ARGS.cluster_id)
    else:
        cluster_id = ''
    if ARGS.pre_args:
        pre_args = '{};'.format(ARGS.pre_args)
    else:
        pre_args = ''

    try:
        # test to see if a controller already exists.
        IPython.parallel.Client(cluster_id=ARGS.cluster_id)
        log('Connecting to existing controller...')
    except IOError:
        log('Starting up new controller...')
        # if not, the Client constructor throws an IOError.
        controller = threading.Thread(target=lambda: os.system(
                'ipcontroller --ip=* {} {} {}'.format(work_dir, cluster_id, q)))
        controller.daemon = True
        controller.start()
        # wait for the controller to load
        time.sleep(1)

    # start engines
    if ARGS.launcher == 'ssh':
        # parse nodefile
        hosts = []
        with open(nodefile) as f:
            for line in f:
                host = line.strip()
                hosts.append(host)
        command = ('ssh {host} "{pre_args} {ipengine} {work_dir} ' +
                   '{cluster_id} {quiet}"')
        for host in hosts:
            engine = threading.Thread(target=lambda: os.system(command.format(
                host=host, pre_args=pre_args,
                ipengine=find_executable('ipengine'), work_dir=work_dir,
                cluster_id=cluster_id, quiet=q)))
            engine.daemon = True
            engine.start()
            time.sleep(1)  # spread out ssh connections
    elif ARGS.launcher == 'mpi':
        engine = threading.Thread(target=lambda: os.system(
            ('mpiexec -machinefile {nodefile} '
             '{ipengine} {clusterid} {quiet} --mpi=mpi4py')
            .format(nodefile=nodefile, work_dir=work_dir,
                    ipengine=find_executable('ipengine'),
                    clusterid=cluster_id, quiet=q)))
        engine.daemon = True
        engine.start()
    else:
        raise NotImplementedError()

    # wait for engines to load
    time.sleep(10)

    if own_nodefile:
        os.unlink(nodefile)

    if ARGS.daemonize:
        daemonize()

    while True:
        time.sleep(1)

if __name__ == '__main__':
    main()
